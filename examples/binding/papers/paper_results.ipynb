{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c436152",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_s1 = \"sup_table_s1.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1efc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_s1 = pd.read_excel(fp_s1)\n",
    "# rename second column to \"Task\"\n",
    "table_s1.rename(columns={table_s1.columns[1]: \"Task\"}, inplace=True)\n",
    "# rename Fl to FI\n",
    "table_s1.rename(columns={\"Fl\": \"FI\"}, inplace=True)\n",
    "table_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27286b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each of the columns Precision, Recall, FI, and MCC to mean and std separate columns\n",
    "def split_mean_std(df, col):\n",
    "    df[[f\"{col}_mean\", f\"{col}_std\"]] = df[col].str.split(\"Â±\", expand=True)\n",
    "    df[f\"{col}_mean\"] = pd.to_numeric(df[f\"{col}_mean\"])\n",
    "    df[f\"{col}_std\"] = pd.to_numeric(df[f\"{col}_std\"])\n",
    "    return df\n",
    "\n",
    "table_s1 = split_mean_std(table_s1, \"Precision\")\n",
    "table_s1 = split_mean_std(table_s1, \"Recall\")\n",
    "table_s1 = split_mean_std(table_s1, \"FI\")\n",
    "table_s1 = split_mean_std(table_s1, \"MCC\")\n",
    "\n",
    "# drop the original columns\n",
    "table_s1.drop(columns=[\"Precision\", \"Recall\", \"FI\", \"MCC\"], inplace=True)\n",
    "\n",
    "table_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a242c9",
   "metadata": {},
   "source": [
    "Validation (DevSet1014), test (TestSet300), and a new\n",
    "independent test set of proteins added to BioLiP after November 2019 and non-\n",
    "redundant in itself and to the other two sets (TestSetNew46)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c34b1",
   "metadata": {},
   "source": [
    "### Performance on different test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance for Precision for Set and task and add error bars for std\n",
    "\n",
    "fig = px.line(\n",
    "    table_s1,\n",
    "    x=\"Task\",\n",
    "    y=\"Precision_mean\",\n",
    "    error_y=\"Precision_std\",\n",
    "    color=\"Set\",\n",
    "    markers=True,\n",
    "    title=\"Precision for Set and task\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance for Precision for Set and task and add error bars for std\n",
    "\n",
    "fig = px.line(\n",
    "    table_s1,\n",
    "    x=\"Task\",\n",
    "    y=\"Recall_mean\",\n",
    "    error_y=\"Recall_std\",\n",
    "    color=\"Set\",\n",
    "    markers=True,\n",
    "    title=\"Recall for Set and task\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67eb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance for Precision for Set and task and add error bars for std\n",
    "\n",
    "fig = px.line(\n",
    "    table_s1,\n",
    "    x=\"Task\",\n",
    "    y=\"FI_mean\",\n",
    "    error_y=\"FI_std\",\n",
    "    color=\"Set\",\n",
    "    markers=True,\n",
    "    title=\"FI for Set and task\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500470b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance for Precision for Set and task and add error bars for std\n",
    "\n",
    "fig = px.line(\n",
    "    table_s1,\n",
    "    x=\"Task\",\n",
    "    y=\"MCC_mean\",\n",
    "    error_y=\"MCC_std\",\n",
    "    color=\"Set\",\n",
    "    markers=True,\n",
    "    title=\"MCC for Set and task\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10))\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d781f7",
   "metadata": {},
   "source": [
    "### Performance for TestSet300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Define metrics and colors\n",
    "colors = [\"#9B1B30\", \"#E6CBA8\", \"#2A4D2D\"]\n",
    "metrics = [\"Precision_mean\", \"Recall_mean\", \"FI_mean\"]\n",
    "\n",
    "# Add traces for Precision, Recall, and FI on the primary y-axis\n",
    "for metric, color in zip(metrics, colors):\n",
    "    df_filtered = table_s1[table_s1[\"Set\"] == \"TestSet300\"]  # Ensure only relevant data is used\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_filtered[\"Task\"],\n",
    "            y=df_filtered[metric],\n",
    "            mode=\"lines+markers\",\n",
    "            name=metric.split(\"_\")[0],  # Shorten legend names\n",
    "            marker=dict(size=10),\n",
    "            line=dict(color=color),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "# Add MCC on the secondary y-axis\n",
    "df_mcc = table_s1[table_s1[\"Set\"] == \"TestSet300\"]  # Filter for consistency\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_mcc[\"Task\"],\n",
    "        y=df_mcc[\"MCC_mean\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"MCC\",\n",
    "        marker=dict(size=10),\n",
    "        line=dict(color=\"#3A5E9D\"),\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Performance for TestSet300\",\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"},\n",
    "    legend_title_text=\"Metrics\",\n",
    ")\n",
    "\n",
    "# Update y-axes\n",
    "fig.update_yaxes(title_text=\"Performance\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"MCC\", secondary_y=True, range=[0, 1], showgrid=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Precision\", \"Recall\", \"FI\"]\n",
    "colors = [\"#9B1B30\", \"#E6CBA8\", \"#2A4D2D\"]\n",
    "\n",
    "# Add traces for Precision, Recall, and FI (with error bars) on the primary y-axis\n",
    "for metric, color in zip(metrics, colors):\n",
    "    df_filtered = table_s1[table_s1[\"Set\"] == \"TestSet300\"]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_filtered[\"Task\"],\n",
    "            y=df_filtered[f\"{metric}_mean\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=metric,\n",
    "            marker=dict(size=10),\n",
    "            line=dict(color=color),\n",
    "            error_y=dict(\n",
    "                type=\"data\",\n",
    "                array=df_filtered[f\"{metric}_std\"],  # Standard deviation column\n",
    "                color=\"grey\",\n",
    "                thickness=1,  # Adjust thickness if needed\n",
    "            ),\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "# Add MCC with error bars on the secondary y-axis\n",
    "df_mcc = table_s1[table_s1[\"Set\"] == \"TestSet300\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_mcc[\"Task\"],\n",
    "        y=df_mcc[\"MCC_mean\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"MCC\",\n",
    "        marker=dict(size=10),\n",
    "        line=dict(color=\"#3A5E9D\"),\n",
    "        error_y=dict(\n",
    "            type=\"data\",\n",
    "            array=df_mcc[\"MCC_std\"],  # Standard deviation for MCC\n",
    "            color=\"grey\",\n",
    "            thickness=1,\n",
    "        ),\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Performance for TestSet300\",\n",
    "    template=\"plotly_white\",\n",
    "    font={\"family\": \"Inter\", \"color\": \"black\"},\n",
    "    legend_title_text=\"Metrics\",\n",
    ")\n",
    "\n",
    "# Update y-axes\n",
    "fig.update_yaxes(title_text=\"Performance\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"MCC\", secondary_y=True, range=[0, 1], showgrid=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484141de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
